{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import *\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('kaggle datasets download -d uciml/breast-cancer-wisconsin-data -f data.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('data.csv')\n",
    "dataset_df = dataset_df[dataset_df.columns[:-2]]\n",
    "dataset_df.head()\n",
    "diagnosis = dataset_df.pop('diagnosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_p = dataset_df[:]\n",
    "dataset_df_p /= dataset_df_p.max(axis=0)\n",
    "# means = dataset_df.mean(axis=0)\n",
    "# dataset_df = dataset_df - dataset_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select heathy people data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_FEATURES = ['radius_mean']\n",
    "DONT_REMOVE_FEATURES = 'mean'\n",
    "FEATURES = [feature for feature in dataset_df_p.columns if \\\n",
    "                (feature not in REMOVE_FEATURES) and (DONT_REMOVE_FEATURES in feature)]\n",
    "N_FEATURES = len(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_healthy = dataset_df_p[diagnosis == 'B']\n",
    "df_unhealthy = dataset_df_p[diagnosis == 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.009339</td>\n",
       "      <td>0.481679</td>\n",
       "      <td>0.365580</td>\n",
       "      <td>0.463979</td>\n",
       "      <td>0.226429</td>\n",
       "      <td>0.598470</td>\n",
       "      <td>0.235350</td>\n",
       "      <td>0.156139</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.620066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077078</td>\n",
       "      <td>0.419256</td>\n",
       "      <td>0.388777</td>\n",
       "      <td>0.396895</td>\n",
       "      <td>0.167184</td>\n",
       "      <td>0.646900</td>\n",
       "      <td>0.167580</td>\n",
       "      <td>0.190895</td>\n",
       "      <td>0.442612</td>\n",
       "      <td>0.448478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.009339</td>\n",
       "      <td>0.465315</td>\n",
       "      <td>0.399949</td>\n",
       "      <td>0.454271</td>\n",
       "      <td>0.207917</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.367690</td>\n",
       "      <td>0.107029</td>\n",
       "      <td>0.154573</td>\n",
       "      <td>0.647039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081267</td>\n",
       "      <td>0.402331</td>\n",
       "      <td>0.413605</td>\n",
       "      <td>0.382524</td>\n",
       "      <td>0.148213</td>\n",
       "      <td>0.589398</td>\n",
       "      <td>0.262382</td>\n",
       "      <td>0.150958</td>\n",
       "      <td>0.250275</td>\n",
       "      <td>0.479663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.009339</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.316701</td>\n",
       "      <td>0.320106</td>\n",
       "      <td>0.109516</td>\n",
       "      <td>0.626683</td>\n",
       "      <td>0.187956</td>\n",
       "      <td>0.069260</td>\n",
       "      <td>0.103181</td>\n",
       "      <td>0.597039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099464</td>\n",
       "      <td>0.283851</td>\n",
       "      <td>0.316108</td>\n",
       "      <td>0.259275</td>\n",
       "      <td>0.074024</td>\n",
       "      <td>0.594789</td>\n",
       "      <td>0.108507</td>\n",
       "      <td>0.070823</td>\n",
       "      <td>0.213986</td>\n",
       "      <td>0.369087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.463536</td>\n",
       "      <td>0.468941</td>\n",
       "      <td>0.438249</td>\n",
       "      <td>0.209436</td>\n",
       "      <td>0.549755</td>\n",
       "      <td>0.109033</td>\n",
       "      <td>0.060028</td>\n",
       "      <td>0.145278</td>\n",
       "      <td>0.482566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059551</td>\n",
       "      <td>0.369034</td>\n",
       "      <td>0.460436</td>\n",
       "      <td>0.336226</td>\n",
       "      <td>0.128326</td>\n",
       "      <td>0.435804</td>\n",
       "      <td>0.043658</td>\n",
       "      <td>0.038602</td>\n",
       "      <td>0.172268</td>\n",
       "      <td>0.299337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.094054</td>\n",
       "      <td>0.291569</td>\n",
       "      <td>0.428717</td>\n",
       "      <td>0.274324</td>\n",
       "      <td>0.080728</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.172061</td>\n",
       "      <td>0.037207</td>\n",
       "      <td>0.029409</td>\n",
       "      <td>0.581908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086528</td>\n",
       "      <td>0.248724</td>\n",
       "      <td>0.443278</td>\n",
       "      <td>0.227946</td>\n",
       "      <td>0.056935</td>\n",
       "      <td>0.582659</td>\n",
       "      <td>0.128261</td>\n",
       "      <td>0.054952</td>\n",
       "      <td>0.088110</td>\n",
       "      <td>0.467761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "19  0.009339     0.481679      0.365580        0.463979   0.226429   \n",
       "20  0.009339     0.465315      0.399949        0.454271   0.207917   \n",
       "21  0.009339     0.338100      0.316701        0.320106   0.109516   \n",
       "37  0.000938     0.463536      0.468941        0.438249   0.209436   \n",
       "46  0.094054     0.291569      0.428717        0.274324   0.080728   \n",
       "\n",
       "    smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "19         0.598470          0.235350        0.156139             0.237624   \n",
       "20         0.657895          0.367690        0.107029             0.154573   \n",
       "21         0.626683          0.187956        0.069260             0.103181   \n",
       "37         0.549755          0.109033        0.060028             0.145278   \n",
       "46         0.526316          0.172061        0.037207             0.029409   \n",
       "\n",
       "    symmetry_mean       ...        fractal_dimension_se  radius_worst  \\\n",
       "19       0.620066       ...                    0.077078      0.419256   \n",
       "20       0.647039       ...                    0.081267      0.402331   \n",
       "21       0.597039       ...                    0.099464      0.283851   \n",
       "37       0.482566       ...                    0.059551      0.369034   \n",
       "46       0.581908       ...                    0.086528      0.248724   \n",
       "\n",
       "    texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "19       0.388777         0.396895    0.167184          0.646900   \n",
       "20       0.413605         0.382524    0.148213          0.589398   \n",
       "21       0.316108         0.259275    0.074024          0.594789   \n",
       "37       0.460436         0.336226    0.128326          0.435804   \n",
       "46       0.443278         0.227946    0.056935          0.582659   \n",
       "\n",
       "    compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \n",
       "19           0.167580         0.190895              0.442612        0.448478  \n",
       "20           0.262382         0.150958              0.250275        0.479663  \n",
       "21           0.108507         0.070823              0.213986        0.369087  \n",
       "37           0.043658         0.038602              0.172268        0.299337  \n",
       "46           0.128261         0.054952              0.088110        0.467761  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_healthy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.8\n",
    "x_train = df_healthy[FEATURES].head(int(len(df_healthy) * train_percent)).values\n",
    "y_train = df_healthy[[REMOVE_FEATURES[0]]].head(int(len(df_healthy) * train_percent)).values\n",
    "\n",
    "x_val = df_healthy[FEATURES].head(int(len(df_healthy) * (1 - train_percent))).values\n",
    "y_val = df_healthy[[REMOVE_FEATURES[0]]].head(int(len(df_healthy) * (1 - train_percent))).values\n",
    "\n",
    "x_test = df_unhealthy[FEATURES].values\n",
    "y_test = df_unhealthy[[REMOVE_FEATURES[0]]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an ensemble of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(n_hidden):\n",
    "    i = Input((N_FEATURES, ))\n",
    "    h = Dense(n_hidden, kernel_initializer='normal', use_bias=True)(i)\n",
    "    h = LeakyReLU()(h)\n",
    "    o = Dense(1)(h)\n",
    "    return Model(i, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "N_MODELS = 5\n",
    "n_hidden_neurons = 32\n",
    "for _ in range(N_MODELS):\n",
    "    models.append(create_base_model(n_hidden_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 32)                320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 353\n",
      "Trainable params: 353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, lr, batch_size, epochs=50, history=None):\n",
    "    current_epoch = 0 if history is None else len(history.history['loss'])\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=keras.optimizers.Adam(lr=lr),\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    \n",
    "    new_history = model.fit(\n",
    "        train_data[0], train_data[1], epochs=current_epoch+epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=val_data,\n",
    "        initial_epoch=current_epoch,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    if history is not None:\n",
    "        for key in new_history.history:\n",
    "            history.history[key].extend(new_history.history[key])\n",
    "    else:\n",
    "        history = new_history\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 285 samples, validate on 71 samples\n",
      "Epoch 1/50\n",
      "285/285 [==============================] - 2s 8ms/step - loss: 0.3452 - mean_squared_error: 0.3452 - val_loss: 0.0278 - val_mean_squared_error: 0.0278\n",
      "Epoch 2/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.1054 - mean_squared_error: 0.1054 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 3/50\n",
      "285/285 [==============================] - 0s 45us/step - loss: 0.0590 - mean_squared_error: 0.0590 - val_loss: 0.1994 - val_mean_squared_error: 0.1994\n",
      "Epoch 4/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.1421 - mean_squared_error: 0.1421 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 5/50\n",
      "285/285 [==============================] - 0s 50us/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 6/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 7/50\n",
      "285/285 [==============================] - 0s 50us/step - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 8/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
      "Epoch 9/50\n",
      "285/285 [==============================] - 0s 32us/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0131 - val_mean_squared_error: 0.0131\n",
      "Epoch 10/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 11/50\n",
      "285/285 [==============================] - 0s 23us/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
      "Epoch 12/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0129 - mean_squared_error: 0.0129 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 13/50\n",
      "285/285 [==============================] - 0s 25us/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 14/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 15/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
      "Epoch 16/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "Epoch 17/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 18/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 19/50\n",
      "285/285 [==============================] - 0s 22us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 20/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 21/50\n",
      "285/285 [==============================] - 0s 48us/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 22/50\n",
      "285/285 [==============================] - 0s 44us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 23/50\n",
      "285/285 [==============================] - 0s 32us/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 24/50\n",
      "285/285 [==============================] - 0s 46us/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 25/50\n",
      "285/285 [==============================] - 0s 32us/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 26/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 27/50\n",
      "285/285 [==============================] - 0s 21us/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 28/50\n",
      "285/285 [==============================] - 0s 29us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 29/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 30/50\n",
      "285/285 [==============================] - 0s 25us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 31/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 32/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 9.3758e-04 - mean_squared_error: 9.3758e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 33/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 7.5279e-04 - val_mean_squared_error: 7.5279e-04\n",
      "Epoch 34/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 8.0518e-04 - mean_squared_error: 8.0518e-04 - val_loss: 6.0659e-04 - val_mean_squared_error: 6.0659e-04\n",
      "Epoch 35/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 5.5851e-04 - mean_squared_error: 5.5851e-04 - val_loss: 6.9477e-04 - val_mean_squared_error: 6.9477e-04\n",
      "Epoch 36/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 5.2360e-04 - mean_squared_error: 5.2360e-04 - val_loss: 4.7771e-04 - val_mean_squared_error: 4.7771e-04\n",
      "Epoch 37/50\n",
      "285/285 [==============================] - 0s 29us/step - loss: 4.6633e-04 - mean_squared_error: 4.6633e-04 - val_loss: 4.6479e-04 - val_mean_squared_error: 4.6479e-04\n",
      "Epoch 38/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 3.7104e-04 - mean_squared_error: 3.7104e-04 - val_loss: 1.7969e-04 - val_mean_squared_error: 1.7969e-04\n",
      "Epoch 39/50\n",
      "285/285 [==============================] - ETA: 0s - loss: 1.9153e-04 - mean_squared_error: 1.9153e- - 0s 39us/step - loss: 2.1792e-04 - mean_squared_error: 2.1792e-04 - val_loss: 1.3606e-04 - val_mean_squared_error: 1.3606e-04\n",
      "Epoch 40/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 1.9048e-04 - mean_squared_error: 1.9048e-04 - val_loss: 1.3646e-04 - val_mean_squared_error: 1.3646e-04\n",
      "Epoch 41/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 1.5841e-04 - mean_squared_error: 1.5841e-04 - val_loss: 9.9992e-05 - val_mean_squared_error: 9.9992e-05\n",
      "Epoch 42/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 1.2768e-04 - mean_squared_error: 1.2768e-04 - val_loss: 1.2351e-04 - val_mean_squared_error: 1.2351e-04\n",
      "Epoch 43/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 1.1826e-04 - mean_squared_error: 1.1826e-04 - val_loss: 1.1402e-04 - val_mean_squared_error: 1.1402e-04\n",
      "Epoch 44/50\n",
      "285/285 [==============================] - ETA: 0s - loss: 1.2418e-04 - mean_squared_error: 1.2418e- - 0s 46us/step - loss: 1.1495e-04 - mean_squared_error: 1.1495e-04 - val_loss: 1.2464e-04 - val_mean_squared_error: 1.2464e-04\n",
      "Epoch 45/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 1.1660e-04 - mean_squared_error: 1.1660e-04 - val_loss: 1.0193e-04 - val_mean_squared_error: 1.0193e-04\n",
      "Epoch 46/50\n",
      "285/285 [==============================] - 0s 46us/step - loss: 9.2402e-05 - mean_squared_error: 9.2402e-05 - val_loss: 9.8753e-05 - val_mean_squared_error: 9.8753e-05\n",
      "Epoch 47/50\n",
      "285/285 [==============================] - 0s 53us/step - loss: 8.7838e-05 - mean_squared_error: 8.7838e-05 - val_loss: 1.1080e-04 - val_mean_squared_error: 1.1080e-04\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 60us/step - loss: 8.8842e-05 - mean_squared_error: 8.8842e-05 - val_loss: 9.6540e-05 - val_mean_squared_error: 9.6540e-05\n",
      "Epoch 49/50\n",
      "285/285 [==============================] - 0s 60us/step - loss: 8.5445e-05 - mean_squared_error: 8.5445e-05 - val_loss: 9.2378e-05 - val_mean_squared_error: 9.2378e-05\n",
      "Epoch 50/50\n",
      "285/285 [==============================] - 0s 68us/step - loss: 7.2844e-05 - mean_squared_error: 7.2844e-05 - val_loss: 7.2572e-05 - val_mean_squared_error: 7.2572e-05\n",
      "Train on 285 samples, validate on 71 samples\n",
      "Epoch 1/50\n",
      "285/285 [==============================] - 2s 8ms/step - loss: 0.7958 - mean_squared_error: 0.7958 - val_loss: 0.0954 - val_mean_squared_error: 0.0954\n",
      "Epoch 2/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.1724 - mean_squared_error: 0.1724 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 3/50\n",
      "285/285 [==============================] - 0s 27us/step - loss: 0.5664 - mean_squared_error: 0.5664 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 4/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0668 - mean_squared_error: 0.0668 - val_loss: 0.3890 - val_mean_squared_error: 0.3890\n",
      "Epoch 5/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.2961 - mean_squared_error: 0.2961 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 6/50\n",
      "285/285 [==============================] - 0s 27us/step - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 7/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0625 - mean_squared_error: 0.0625 - val_loss: 0.0818 - val_mean_squared_error: 0.0818\n",
      "Epoch 8/50\n",
      "285/285 [==============================] - 0s 30us/step - loss: 0.0808 - mean_squared_error: 0.0808 - val_loss: 0.0125 - val_mean_squared_error: 0.0125\n",
      "Epoch 9/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 10/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0783 - val_mean_squared_error: 0.0783\n",
      "Epoch 11/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0548 - mean_squared_error: 0.0548 - val_loss: 0.0209 - val_mean_squared_error: 0.0209\n",
      "Epoch 12/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.0170 - val_mean_squared_error: 0.0170\n",
      "Epoch 13/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
      "Epoch 14/50\n",
      "285/285 [==============================] - 0s 23us/step - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 15/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0301 - val_mean_squared_error: 0.0301\n",
      "Epoch 16/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0149 - val_mean_squared_error: 0.0149\n",
      "Epoch 17/50\n",
      "285/285 [==============================] - 0s 22us/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 18/50\n",
      "285/285 [==============================] - 0s 29us/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "Epoch 19/50\n",
      "285/285 [==============================] - 0s 44us/step - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 20/50\n",
      "285/285 [==============================] - 0s 23us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
      "Epoch 21/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
      "Epoch 22/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 23/50\n",
      "285/285 [==============================] - 0s 51us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 24/50\n",
      "285/285 [==============================] - 0s 25us/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 25/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 26/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 27/50\n",
      "285/285 [==============================] - 0s 44us/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 28/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 29/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 30/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 31/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 32/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 33/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 34/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 35/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 36/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 37/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 38/50\n",
      "285/285 [==============================] - 0s 26us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 39/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 40/50\n",
      "285/285 [==============================] - 0s 32us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 41/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 42/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 43/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 44/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 45/50\n",
      "285/285 [==============================] - 0s 23us/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 46/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 9.0640e-04 - mean_squared_error: 9.0640e-04 - val_loss: 9.6694e-04 - val_mean_squared_error: 9.6694e-04\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 30us/step - loss: 8.8524e-04 - mean_squared_error: 8.8524e-04 - val_loss: 9.2376e-04 - val_mean_squared_error: 9.2376e-04\n",
      "Epoch 48/50\n",
      "285/285 [==============================] - 0s 21us/step - loss: 7.7203e-04 - mean_squared_error: 7.7203e-04 - val_loss: 9.6422e-04 - val_mean_squared_error: 9.6422e-04\n",
      "Epoch 49/50\n",
      "285/285 [==============================] - 0s 20us/step - loss: 7.4319e-04 - mean_squared_error: 7.4319e-04 - val_loss: 8.1460e-04 - val_mean_squared_error: 8.1460e-04\n",
      "Epoch 50/50\n",
      "285/285 [==============================] - 0s 20us/step - loss: 6.6360e-04 - mean_squared_error: 6.6360e-04 - val_loss: 6.8913e-04 - val_mean_squared_error: 6.8913e-04\n",
      "Train on 285 samples, validate on 71 samples\n",
      "Epoch 1/50\n",
      "285/285 [==============================] - 2s 8ms/step - loss: 0.3648 - mean_squared_error: 0.3648 - val_loss: 0.0133 - val_mean_squared_error: 0.0133\n",
      "Epoch 2/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0634 - mean_squared_error: 0.0634 - val_loss: 0.1916 - val_mean_squared_error: 0.1916\n",
      "Epoch 3/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.1478 - mean_squared_error: 0.1478 - val_loss: 0.0940 - val_mean_squared_error: 0.0940\n",
      "Epoch 4/50\n",
      "285/285 [==============================] - 0s 55us/step - loss: 0.0979 - mean_squared_error: 0.0979 - val_loss: 0.0778 - val_mean_squared_error: 0.0778\n",
      "Epoch 5/50\n",
      "285/285 [==============================] - 0s 48us/step - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "Epoch 6/50\n",
      "285/285 [==============================] - 0s 31us/step - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 7/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
      "Epoch 8/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 9/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 10/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "Epoch 11/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 12/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 13/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 14/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 15/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 16/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0145 - val_mean_squared_error: 0.0145\n",
      "Epoch 17/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 18/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 19/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 20/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n",
      "Epoch 21/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 22/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 23/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 24/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 25/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 26/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 27/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 28/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 29/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 30/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 31/50\n",
      "285/285 [==============================] - ETA: 0s - loss: 0.0013 - mean_squared_error: 0.00 - 0s 45us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 32/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 33/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 34/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 35/50\n",
      "285/285 [==============================] - 0s 55us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 9.1697e-04 - val_mean_squared_error: 9.1697e-04\n",
      "Epoch 36/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 8.3734e-04 - mean_squared_error: 8.3734e-04 - val_loss: 8.5783e-04 - val_mean_squared_error: 8.5783e-04\n",
      "Epoch 37/50\n",
      "285/285 [==============================] - 0s 48us/step - loss: 8.4125e-04 - mean_squared_error: 8.4125e-04 - val_loss: 9.5896e-04 - val_mean_squared_error: 9.5896e-04\n",
      "Epoch 38/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 7.5001e-04 - mean_squared_error: 7.5001e-04 - val_loss: 7.4903e-04 - val_mean_squared_error: 7.4903e-04\n",
      "Epoch 39/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 6.0756e-04 - mean_squared_error: 6.0756e-04 - val_loss: 5.6521e-04 - val_mean_squared_error: 5.6521e-04\n",
      "Epoch 40/50\n",
      "285/285 [==============================] - 0s 50us/step - loss: 5.7223e-04 - mean_squared_error: 5.7223e-04 - val_loss: 5.5848e-04 - val_mean_squared_error: 5.5848e-04\n",
      "Epoch 41/50\n",
      "285/285 [==============================] - 0s 46us/step - loss: 4.8237e-04 - mean_squared_error: 4.8237e-04 - val_loss: 4.8507e-04 - val_mean_squared_error: 4.8507e-04\n",
      "Epoch 42/50\n",
      "285/285 [==============================] - 0s 50us/step - loss: 3.8757e-04 - mean_squared_error: 3.8757e-04 - val_loss: 3.6837e-04 - val_mean_squared_error: 3.6837e-04\n",
      "Epoch 43/50\n",
      "285/285 [==============================] - 0s 49us/step - loss: 3.9145e-04 - mean_squared_error: 3.9145e-04 - val_loss: 3.3450e-04 - val_mean_squared_error: 3.3450e-04\n",
      "Epoch 44/50\n",
      "285/285 [==============================] - 0s 55us/step - loss: 2.9781e-04 - mean_squared_error: 2.9781e-04 - val_loss: 2.8624e-04 - val_mean_squared_error: 2.8624e-04\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 47us/step - loss: 2.4899e-04 - mean_squared_error: 2.4899e-04 - val_loss: 2.0901e-04 - val_mean_squared_error: 2.0901e-04\n",
      "Epoch 46/50\n",
      "285/285 [==============================] - 0s 51us/step - loss: 2.3210e-04 - mean_squared_error: 2.3210e-04 - val_loss: 2.1020e-04 - val_mean_squared_error: 2.1020e-04\n",
      "Epoch 47/50\n",
      "285/285 [==============================] - 0s 48us/step - loss: 1.9149e-04 - mean_squared_error: 1.9149e-04 - val_loss: 1.4449e-04 - val_mean_squared_error: 1.4449e-04\n",
      "Epoch 48/50\n",
      "285/285 [==============================] - 0s 47us/step - loss: 1.5390e-04 - mean_squared_error: 1.5390e-04 - val_loss: 1.2501e-04 - val_mean_squared_error: 1.2501e-04\n",
      "Epoch 49/50\n",
      "285/285 [==============================] - 0s 46us/step - loss: 1.4242e-04 - mean_squared_error: 1.4242e-04 - val_loss: 1.5848e-04 - val_mean_squared_error: 1.5848e-04\n",
      "Epoch 50/50\n",
      "285/285 [==============================] - 0s 56us/step - loss: 1.3457e-04 - mean_squared_error: 1.3457e-04 - val_loss: 9.4317e-05 - val_mean_squared_error: 9.4317e-05\n",
      "Train on 285 samples, validate on 71 samples\n",
      "Epoch 1/50\n",
      "285/285 [==============================] - 2s 8ms/step - loss: 1.1929 - mean_squared_error: 1.1929 - val_loss: 0.2565 - val_mean_squared_error: 0.2565\n",
      "Epoch 2/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.2085 - mean_squared_error: 0.2085 - val_loss: 0.7077 - val_mean_squared_error: 0.7077\n",
      "Epoch 3/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.6056 - mean_squared_error: 0.6056 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 4/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.0490 - mean_squared_error: 0.0490 - val_loss: 0.2419 - val_mean_squared_error: 0.2419\n",
      "Epoch 5/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.2198 - mean_squared_error: 0.2198 - val_loss: 0.1746 - val_mean_squared_error: 0.1746\n",
      "Epoch 6/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.1129 - mean_squared_error: 0.1129 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 7/50\n",
      "285/285 [==============================] - 0s 50us/step - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.1089 - val_mean_squared_error: 0.1089\n",
      "Epoch 8/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.1180 - mean_squared_error: 0.1180 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 9/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 10/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.0489 - mean_squared_error: 0.0489 - val_loss: 0.1152 - val_mean_squared_error: 0.1152\n",
      "Epoch 11/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.0765 - mean_squared_error: 0.0765 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 12/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 13/50\n",
      "285/285 [==============================] - 0s 50us/step - loss: 0.0568 - mean_squared_error: 0.0568 - val_loss: 0.0128 - val_mean_squared_error: 0.0128\n",
      "Epoch 14/50\n",
      "285/285 [==============================] - 0s 46us/step - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 15/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0284 - val_mean_squared_error: 0.0284\n",
      "Epoch 16/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "Epoch 17/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0151 - mean_squared_error: 0.0151 - val_loss: 0.0142 - val_mean_squared_error: 0.0142\n",
      "Epoch 18/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0142 - mean_squared_error: 0.0142 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 19/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "Epoch 20/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 21/50\n",
      "285/285 [==============================] - 0s 27us/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "Epoch 22/50\n",
      "285/285 [==============================] - 0s 31us/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 23/50\n",
      "285/285 [==============================] - 0s 54us/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "Epoch 24/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 25/50\n",
      "285/285 [==============================] - 0s 45us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 26/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 27/50\n",
      "285/285 [==============================] - 0s 32us/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 28/50\n",
      "285/285 [==============================] - 0s 45us/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 29/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 30/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 31/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 32/50\n",
      "285/285 [==============================] - 0s 49us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 33/50\n",
      "285/285 [==============================] - 0s 44us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 34/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 35/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 9.9376e-04 - val_mean_squared_error: 9.9376e-04\n",
      "Epoch 36/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 37/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 9.8127e-04 - mean_squared_error: 9.8127e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 38/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 9.1544e-04 - val_mean_squared_error: 9.1544e-04\n",
      "Epoch 39/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 6.7735e-04 - mean_squared_error: 6.7735e-04 - val_loss: 8.1626e-04 - val_mean_squared_error: 8.1626e-04\n",
      "Epoch 40/50\n",
      "285/285 [==============================] - 0s 45us/step - loss: 8.8480e-04 - mean_squared_error: 8.8480e-04 - val_loss: 6.1664e-04 - val_mean_squared_error: 6.1664e-04\n",
      "Epoch 41/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 5.9059e-04 - mean_squared_error: 5.9059e-04 - val_loss: 9.9148e-04 - val_mean_squared_error: 9.9148e-04\n",
      "Epoch 42/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 6.6159e-04 - mean_squared_error: 6.6159e-04 - val_loss: 4.7620e-04 - val_mean_squared_error: 4.7620e-04\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 40us/step - loss: 5.7219e-04 - mean_squared_error: 5.7219e-04 - val_loss: 4.1141e-04 - val_mean_squared_error: 4.1141e-04\n",
      "Epoch 44/50\n",
      "285/285 [==============================] - 0s 28us/step - loss: 4.5325e-04 - mean_squared_error: 4.5325e-04 - val_loss: 6.8488e-04 - val_mean_squared_error: 6.8488e-04\n",
      "Epoch 45/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 4.8875e-04 - mean_squared_error: 4.8875e-04 - val_loss: 3.3638e-04 - val_mean_squared_error: 3.3638e-04\n",
      "Epoch 46/50\n",
      "285/285 [==============================] - 0s 46us/step - loss: 3.8500e-04 - mean_squared_error: 3.8500e-04 - val_loss: 3.1030e-04 - val_mean_squared_error: 3.1030e-04\n",
      "Epoch 47/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 3.5790e-04 - mean_squared_error: 3.5790e-04 - val_loss: 4.3253e-04 - val_mean_squared_error: 4.3253e-04\n",
      "Epoch 48/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 3.3868e-04 - mean_squared_error: 3.3868e-04 - val_loss: 2.6193e-04 - val_mean_squared_error: 2.6193e-04\n",
      "Epoch 49/50\n",
      "285/285 [==============================] - 0s 44us/step - loss: 2.7170e-04 - mean_squared_error: 2.7170e-04 - val_loss: 2.3538e-04 - val_mean_squared_error: 2.3538e-04\n",
      "Epoch 50/50\n",
      "285/285 [==============================] - 0s 46us/step - loss: 2.7289e-04 - mean_squared_error: 2.7289e-04 - val_loss: 2.9040e-04 - val_mean_squared_error: 2.9040e-04\n",
      "Train on 285 samples, validate on 71 samples\n",
      "Epoch 1/50\n",
      "285/285 [==============================] - 2s 8ms/step - loss: 0.3940 - mean_squared_error: 0.3940 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 2/50\n",
      "285/285 [==============================] - 0s 56us/step - loss: 0.0934 - mean_squared_error: 0.0934 - val_loss: 0.2206 - val_mean_squared_error: 0.2206\n",
      "Epoch 3/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.1718 - mean_squared_error: 0.1718 - val_loss: 0.0750 - val_mean_squared_error: 0.0750\n",
      "Epoch 4/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 0.0895 - mean_squared_error: 0.0895 - val_loss: 0.1301 - val_mean_squared_error: 0.1301\n",
      "Epoch 5/50\n",
      "285/285 [==============================] - 0s 50us/step - loss: 0.0806 - mean_squared_error: 0.0806 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 6/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0634 - val_mean_squared_error: 0.0634\n",
      "Epoch 7/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0645 - mean_squared_error: 0.0645 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
      "Epoch 8/50\n",
      "285/285 [==============================] - 0s 45us/step - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 9/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 10/50\n",
      "285/285 [==============================] - 0s 49us/step - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 11/50\n",
      "285/285 [==============================] - 0s 44us/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0233 - val_mean_squared_error: 0.0233\n",
      "Epoch 12/50\n",
      "285/285 [==============================] - 0s 30us/step - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 13/50\n",
      "285/285 [==============================] - 0s 28us/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0152 - val_mean_squared_error: 0.0152\n",
      "Epoch 14/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0125 - mean_squared_error: 0.0125 - val_loss: 0.0172 - val_mean_squared_error: 0.0172\n",
      "Epoch 15/50\n",
      "285/285 [==============================] - 0s 35us/step - loss: 0.0099 - mean_squared_error: 0.0099 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 16/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "Epoch 17/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 18/50\n",
      "285/285 [==============================] - 0s 21us/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 19/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 20/50\n",
      "285/285 [==============================] - 0s 52us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 21/50\n",
      "285/285 [==============================] - 0s 27us/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 22/50\n",
      "285/285 [==============================] - 0s 39us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 23/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 24/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 25/50\n",
      "285/285 [==============================] - 0s 42us/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 26/50\n",
      "285/285 [==============================] - 0s 41us/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 27/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 28/50\n",
      "285/285 [==============================] - 0s 44us/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 29/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 30/50\n",
      "285/285 [==============================] - 0s 43us/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 31/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 32/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 33/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 34/50\n",
      "285/285 [==============================] - 0s 26us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 35/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 36/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 37/50\n",
      "285/285 [==============================] - 0s 27us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 38/50\n",
      "285/285 [==============================] - 0s 34us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 39/50\n",
      "285/285 [==============================] - 0s 45us/step - loss: 9.3908e-04 - mean_squared_error: 9.3908e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 40/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 9.5833e-04 - mean_squared_error: 9.5833e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 41/50\n",
      "285/285 [==============================] - 0s 33us/step - loss: 7.7659e-04 - mean_squared_error: 7.7659e-04 - val_loss: 7.6240e-04 - val_mean_squared_error: 7.6240e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "285/285 [==============================] - 0s 32us/step - loss: 7.6879e-04 - mean_squared_error: 7.6879e-04 - val_loss: 6.8045e-04 - val_mean_squared_error: 6.8045e-04\n",
      "Epoch 43/50\n",
      "285/285 [==============================] - 0s 37us/step - loss: 6.1120e-04 - mean_squared_error: 6.1120e-04 - val_loss: 8.3384e-04 - val_mean_squared_error: 8.3384e-04\n",
      "Epoch 44/50\n",
      "285/285 [==============================] - 0s 46us/step - loss: 6.0917e-04 - mean_squared_error: 6.0917e-04 - val_loss: 5.1270e-04 - val_mean_squared_error: 5.1270e-04\n",
      "Epoch 45/50\n",
      "285/285 [==============================] - 0s 30us/step - loss: 5.0807e-04 - mean_squared_error: 5.0807e-04 - val_loss: 4.3886e-04 - val_mean_squared_error: 4.3886e-04\n",
      "Epoch 46/50\n",
      "285/285 [==============================] - 0s 49us/step - loss: 4.1698e-04 - mean_squared_error: 4.1698e-04 - val_loss: 5.2884e-04 - val_mean_squared_error: 5.2884e-04\n",
      "Epoch 47/50\n",
      "285/285 [==============================] - 0s 30us/step - loss: 3.9860e-04 - mean_squared_error: 3.9860e-04 - val_loss: 3.2795e-04 - val_mean_squared_error: 3.2795e-04\n",
      "Epoch 48/50\n",
      "285/285 [==============================] - 0s 36us/step - loss: 2.8954e-04 - mean_squared_error: 2.8954e-04 - val_loss: 2.4706e-04 - val_mean_squared_error: 2.4706e-04\n",
      "Epoch 49/50\n",
      "285/285 [==============================] - 0s 40us/step - loss: 2.6115e-04 - mean_squared_error: 2.6115e-04 - val_loss: 2.3733e-04 - val_mean_squared_error: 2.3733e-04\n",
      "Epoch 50/50\n",
      "285/285 [==============================] - 0s 38us/step - loss: 2.1107e-04 - mean_squared_error: 2.1107e-04 - val_loss: 1.8472e-04 - val_mean_squared_error: 1.8472e-04\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "histories = defaultdict(lambda:None)\n",
    "for lr in [0.1]:\n",
    "    for i in range(N_MODELS):\n",
    "        histories[i] = train(models[i], [x_train, y_train], (x_val, y_val), lr, 200, history=histories[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAJCCAYAAACBLyXFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_test_average = []\n",
    "y_pred_val_average = []\n",
    "y_pred_train_average = []\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    y_val_pred = model.predict(x_val)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_pred_test_average.append(y_test_pred)\n",
    "    y_pred_val_average.append(y_val_pred)\n",
    "    y_pred_train_average.append(y_train_pred)\n",
    "\n",
    "\n",
    "y_pred_test_var = np.var(y_pred_test_average, axis=0)\n",
    "y_pred_val_var = np.var(y_pred_val_average, axis=0)\n",
    "y_pred_train_var = np.var(y_pred_train_average, axis=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(y_pred_val_var.flatten(), color='b',  bins=20, label='val')\n",
    "plt.hist(y_pred_test_var.flatten(), ls='dotted', lw=3, fc=(1, 0, 0, 0.5), bins=50, label='test')\n",
    "plt.hist(y_pred_train_var.flatten(), ls='dotted', lw=4, fc=(0, 1, 0, 0.5), bins=20, label='train')\n",
    "plt.legend()\n",
    "# plt.axis('equal')\n",
    "# plt.xlim(plt.xlim())\n",
    "# plt.ylim(plt.ylim())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
